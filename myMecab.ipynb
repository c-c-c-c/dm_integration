{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "myMecab",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-c-c-c/dm_integration/blob/master/myMecab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R8Oh-JGka-8_"
      },
      "source": [
        "# 吉村インテグレーションステップ \n",
        "\n",
        "目的は、ドラマの構成/脚本として、「どのタイミングで」「どんな\n",
        "出来事」が起こると良いかという示唆を見出すこと。\n",
        "\n",
        "★目的変数\n",
        "　　ドラマの初回視聴率からの上下動(%で、閾値を良い、悪い、\n",
        "　　普通になるように３パターン準備)\n",
        "\n",
        "★特徴量\n",
        "　　-ドラマを恋愛、刑事、ヒューマンなどのカテゴリーに分ける\n",
        "　　-さらに、ドラマをステージ(序盤、中盤、終盤)に分ける\n",
        "　　-ドラマ名-ステージを行にした、単語×ドラマステージ行列を作成\n",
        "\n",
        "★モデル\n",
        "　　- 目的変数に対しロッソ回帰を行う。\n",
        "　　- もしくは、教師なしのグルーピングを行う。(kmeans, トピックモデル)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qkpjTGlYJdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P-6WpdMM7ywT",
        "colab": {}
      },
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPKMuQKTFTGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0otnQug1tpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install chardet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ahF2OfXfa-9B",
        "colab": {}
      },
      "source": [
        "import joblib\n",
        "import MeCab\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "import pprint\n",
        "import IPython.core.display as display\n",
        "import IPython.display\n",
        "from scipy.sparse import csr_matrix\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DUHMMXZRa-9E"
      },
      "source": [
        "## データの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YCwAqWLba-9F",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# EPGデータ(手作業での修正中))\n",
        "df_epg = pd.read_excel(\"./drive/My Drive/0_インテグ作業/data/EPG_checking0212.xlsx\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4G9yZISyV6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f= open(\"./drive/My Drive/0_インテグ作業/data/drama_category0220.json\", 'r')\n",
        "\n",
        "drama_category_dic = json.load(f) #JSON形式で読み込む\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie05rJGFdPaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pprint.pprint(drama_category_dic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9FqflXpeGw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f= open(\"./drive/My Drive/0_インテグ作業/data/drama_win_lose.json\", 'r')\n",
        "\n",
        "drama_win_lose_dic = json.load(f) #JSON形式で読み込む\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN9Ikeaa990W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_epg[\"sharp_epg_tknz\"] = np.nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-SFrbJDgbFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ゴミ除去\n",
        "\n",
        "def removeTrash (text):\n",
        "    import re\n",
        "\n",
        "    result_text = text\n",
        "    result_text = re.sub(r\"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\", \"\", result_text)\n",
        "    result_text = re.sub(r\"番組詳細|制作・著作|制作著作\", \"\", result_text)\n",
        "    result_text = re.sub(r\"[!\\(\\)=『』～/]\", \"\", result_text)\n",
        "    result_text = re.sub(r\"フジテレビ|日本テレビ|TBS|テレビ朝日|TBS|関西テレビ\", \"\", result_text)\n",
        "    result_text = re.sub(r\"\\d+\", \"\", result_text)\n",
        "    \n",
        "    result_text = re.sub(r\"【公式.*?】\", \"\", result_text)\n",
        "    result_text = re.sub(r\"\\u3000\", \"\", result_text)\n",
        "\n",
        "    return result_text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIgBCGTUcorI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mecab = MeCab.Tagger()\n",
        "mecab.parse(\"\")\n",
        "for i, text in enumerate( df_epg['sharp_epg_hand_corrected']):\n",
        "    text_tokenized = []\n",
        "\n",
        "    # URL、記号などのゴミを取り除く\n",
        "    if type(text) is not str: \n",
        "        if np.isnan(text) :\n",
        "            continue \n",
        "    text = removeTrash(text)\n",
        "    node = mecab.parseToNode(text)\n",
        "    while node:\n",
        "        node = node.next\n",
        "        if node is None:\n",
        "            continue\n",
        "\n",
        "        if not node.feature.startswith(\"BOS/EOS\") and not node.feature.startswith(\"助詞\") and\\\n",
        "            not node.feature.startswith(\"記号\") and\\\n",
        "            node.feature.find(\"人名\") == -1 and\\\n",
        "            not node.feature.startswith(\"助動詞\"):\n",
        "            text_tokenized.append(node.surface)\n",
        "\n",
        "    df_epg[\"sharp_epg_tknz\"].iloc[i] = text_tokenized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkohrTp-ot6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 手作業での前処理が完了してないので、完了したものだけにする\n",
        "bool_list = [] \n",
        "\n",
        "for i in range(len(df_epg[\"sharp_epg_hand_corrected\"])):\n",
        "    bool_list.append( type( df_epg[\"sharp_epg_tknz\"].iloc[i] ) != float )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBaS8qJ3xOfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_notnull =  df_epg[bool_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqLRG7vYDeob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_notnull[\"phys_cnt\"] = np.nan\n",
        "df_notnull[\"category\"] = np.nan\n",
        "df_notnull =  df_notnull.rename(columns={ 'Unnamed: 0' : 'sort_id' } )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X2mHTM_DwqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drama_category_dic.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdaTOtJhtd0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 物理カウントとカテゴリーを加える\n",
        "love_cnt = 0\n",
        "police_cnt = 0\n",
        "for tmp_key  in df_notnull[\"drama_key\"].unique():\n",
        "    cnt = 0\n",
        "\n",
        "    if tmp_key in drama_category_dic[\"love\"]:\n",
        "        love_cnt += 1\n",
        "\n",
        "        qry_l = \" drama_key == @tmp_key\"\n",
        "        target_idx = df_notnull.query(qry_l).index\n",
        "        df_notnull[ \"category\" ].loc [target_idx] = \"love\"\n",
        "\n",
        "\n",
        "    if tmp_key in drama_category_dic[\"police\"]:\n",
        "        police_cnt += 1\n",
        "        qry_p = \" drama_key == @tmp_key\"\n",
        "        target_idx = df_notnull.query(qry_p).index\n",
        "        df_notnull[ \"category\" ].loc [target_idx] = \"police\"\n",
        "\n",
        "    for sort_id in  df_notnull[df_notnull[\"drama_key\"] == tmp_key]['sort_id'].values:\n",
        "        cnt  += 1\n",
        "\n",
        "        qry = \" sort_id == @sort_id\"\n",
        "        target_idx = df_notnull.query(qry).index\n",
        "        df_notnull[\"phys_cnt\"].loc[target_idx] = cnt\n",
        "\n",
        "    # print(tmp_key)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo7qH_8UaGAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_stage ステージごとに作る\n",
        "\n",
        "df_stage = pd.DataFrame( columns=['drama_key','drama_title','stages','epg_joined','drama_category','win_lose'] )\n",
        "\n",
        "\n",
        "for tmp_key in df_notnull.drama_key.unique():\n",
        "\n",
        "    win_or_lose = \"\"\n",
        "    if tmp_key in drama_win_lose_dic[\"win\"]:\n",
        "        win_or_lose = \"win\"\n",
        "    elif tmp_key in drama_win_lose_dic[\"draw\"]:\n",
        "        win_or_lose = \"draw\"\n",
        "    elif tmp_key in drama_win_lose_dic[\"lose\"]:\n",
        "        win_or_lose = \"lose\"\n",
        "    else:\n",
        "        print(\"????\")\n",
        "\n",
        "    tmp_cat = str(df_notnull[ df_notnull[\"drama_key\"] == tmp_key ].category.values[0])\n",
        "    tmp_title = str(df_notnull[ df_notnull[\"drama_key\"] == tmp_key ].drama_title.values[0])\n",
        "\n",
        "    for tmp_stage in [\"early\",\"middle\" ,\"late\"]:\n",
        "\n",
        "        qry = \"\"\n",
        "        if tmp_stage == \"early\":\n",
        "            qry = \"drama_key == @tmp_key & phys_cnt < 4 \"\n",
        "\n",
        "        elif tmp_stage == \"middle\":\n",
        "            qry = \"drama_key == @tmp_key & 4< phys_cnt < 7 \"\n",
        "\n",
        "        else:\n",
        "            qry = \"drama_key == @tmp_key & phys_cnt >= 7 \"\n",
        "\n",
        "        target_i =  df_notnull.query(qry)[\"sharp_epg_hand_corrected\"]\n",
        "        epg_joined = str(df_notnull.query(qry)[\"sharp_epg_hand_corrected\"].values)\n",
        "\n",
        "        tmp_se = pd.Series( [ tmp_key, tmp_title,tmp_stage, epg_joined  ,tmp_cat, win_or_lose ]  , index=df_stage.columns  )\n",
        "        df_stage = df_stage.append( tmp_se, ignore_index=True )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPv0645OcmWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_stage.iloc[1:200]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-KXTCDwj-i_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 空の列を足す\n",
        "df_stage[\"epg_tknz\"] = np.nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MHfP09qq0X1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mecab = MeCab.Tagger()\n",
        "mecab.parse(\"\")\n",
        "for i, text in enumerate( df_stage['epg_joined']):\n",
        "    text_tokenized = []\n",
        "\n",
        "    # URL、記号などのゴミを取り除く\n",
        "    if type(text) is not str: \n",
        "        if np.isnan(text) :\n",
        "            continue \n",
        "    text = removeTrash(text)\n",
        "    node = mecab.parseToNode(text)\n",
        "    while node:\n",
        "        node = node.next\n",
        "        if node is None:\n",
        "            continue\n",
        "\n",
        "        if not node.feature.startswith(\"BOS/EOS\") and not node.feature.startswith(\"助詞\") and\\\n",
        "            not node.feature.startswith(\"記号\") and\\\n",
        "            node.feature.find(\"人名\") == -1 and\\\n",
        "            not node.feature.startswith(\"助動詞\"):\n",
        "            text_tokenized.append(node.surface)\n",
        "\n",
        "    df_stage[\"epg_tknz\"].iloc[i] = text_tokenized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDJYZrGkC9SJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_stage[\"epg_tknz\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqBVvhWpOGk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_stage[\"win_lose_dummy\"] = np.nan\n",
        "for i in range(len( df_stage[\"win_lose\"] )):\n",
        "\n",
        "    if df_stage[\"win_lose\"].iloc[i] == \"win\":\n",
        "        df_stage[\"win_lose_dummy\"].iloc[i] = 1\n",
        "\n",
        "    elif df_stage[\"win_lose\"].iloc[i] == \"draw\":\n",
        "        df_stage[\"win_lose_dummy\"].iloc[i] = 0\n",
        "\n",
        "    elif df_stage[\"win_lose\"].iloc[i] == \"lose\":\n",
        "        df_stage[\"win_lose_dummy\"].iloc[i] = -1\n",
        "        \n",
        "    else :\n",
        "        print(\"???\")\n",
        "    # df_stage[\"win_lose_dummy\"] = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RciqfI-qQkzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLGKR-iuJquN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 行列の中で1より大きい数値を１にならす\n",
        "def bool_bow ( argX ) :\n",
        "\n",
        "    # 各要素を見に行って２以上なら１補正\n",
        "    for j in range(argX.shape[1]):\n",
        "        tmp_sum = 0\n",
        "        for i in range(argX.shape[0]):\n",
        "\n",
        "            if argX[i, j] > 1:\n",
        "                argX[i, j] = 1\n",
        "\n",
        "    return argX\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uvjR61pbtAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_rare_words ( argX , pre_vectorizer ) :\n",
        "    for j in range(argX.shape[1]):\n",
        "        for i in range(argX.shape[0]):\n",
        "            if argX[i, j] > 1:\n",
        "                argX[i, j] =   1\n",
        "            \n",
        "    tmp_df_sp = pd.DataFrame(argX.toarray(), columns=[ x[0] for x in sorted(pre_vectorizer.vocabulary_.items(), key=lambda x: x[1]) ])\n",
        "\n",
        "    result_rare_words =[]\n",
        "\n",
        "    for   tmp_word in tmp_df_sp.columns:\n",
        "        tmp_sum = tmp_df_sp[tmp_word].sum()\n",
        "\n",
        "        if  tmp_sum < 5:\n",
        "            result_rare_words.append(tmp_word)\n",
        "    # import pdb; pdb.set_trace()\n",
        "    return result_rare_words\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_o522TtodZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf3tdPlSJ3pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ステージごとに分けた後のdfを引数にとる\n",
        "not_rare_words = []\n",
        "particular_stop_words = [\"いる\", \"おり\", \"arata\",\"uend\", \"c\",\"C\" , \"この\",\"その\" ,\"そう\",\"あり\",\"際\",\\\n",
        "                        \"genking\",\"件\",\"メゾン\",\"しれ\",\"きり\",\"u\",\"U\",\"あり\",\"回\",\\\n",
        "                        \"お\",\"leola\",\"ください\",\"また\",\\\n",
        "                        \"後\",\"ー\",\"a\",\"A\",\"なく\",\"or\",\"OR\",\\\n",
        "                        \"ii\",\"II\",\"まま\",\"皆\",\"百\",\"ーー\",\"j\",\"J\",\"どう\",\"x\",\"X\",\"sit\",\"SIT\",\"通\",\\\n",
        "                        \"TSUTAYA\",\"かつ\",\"無い\",\"性\",\"しまい\",\"不\",\"陽\",\\\n",
        "                        \"性\",\"将\",\"点\",\"結\",\"負っ\",\\\n",
        "                        \"luz\",\"その後\",\"すると\",\"方\",\"屋\",\"ほう\",\"フジオカ\",\"事\",\"二\",\"くれる\",\"leg\",\\\n",
        "                         \"genking\",\"さ\",\"ところ\",\\\n",
        "                         \"太郎\",\"数\",\"すると\",\"あげ\",\"上\",\"takahiro\",\\\n",
        "                        \"み\",\"メゾン\",\"ある\",\"よう\",\"あっ\",\"ほしい\",\"彼ら\",\"状\",\"megumi\",\\\n",
        "                         \"なぜ\",\"上げ\",\"まるで\",\"nima\",\"NIMA\",\"j\",\"ii\",\"その後\",\"ビョンホン\",\"x\",\\\n",
        "                         \"tsutaya\",\"頃\",\"げ\",\"史\",\"これ\",\"あ\",\"ディーン・フジオカ\",\"(ディーン・フジオカ)\"\\\n",
        "                         \"genking\",\"ユリカ\", \"BAR\",\"FUJIOKA\",\"GENKING\",\"ARATA\",\"X\",\"MEGUMI\", \"メゾン・フローラル\", \n",
        "                         \"BANZAI\",\"s\",\"S\",\"一\",\"・フローラル\",\"TAKAHIRO\",\"NAYUTAWAVE\",\"GENKING\",\n",
        "                         \"でき\",\"他\",\"ウイルステロブラッディ\",\"ただ\",\"RECORDS\",\"KAMIYA\",\"GEROGE\",\"K\",\n",
        "                         \"の\",\"ド\",\"たち\",\"ん\",\"ウイルステロブラッディ・マンデイ\",\"ら\",\"が\",\"uFUJIOKA\",\n",
        "                         \"れる\",\"し\",\"何\",\"カヲル\",\"いう\",\"ela\",\"いっ\",\"もと\",\"r\",\"だが\",\"僕\",\"アイ\",\"DEAN\",\n",
        "                         \"メグリン\",\"シュウメイリン\",\"オリヴィエ\",\"き\",\"uEND\",\"かけ\",\"いっ\",\"し\",\"ザ\",\"れる\",\"コンテンポラリーダンサー\",\\\n",
        "                         \"Kis\",\"メグリン\",\"ヴィ\",\"JMT\",\"r\",\"R\",\"smap\",\"SMAP\",\n",
        "                         ]\n",
        "\n",
        "def fit_trans_lasso (df_tmp) :\n",
        "    pre_vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "    Xpre = pre_vectorizer.fit_transform(\n",
        "        [str(i) for i in df_tmp[\"epg_tknz\"].values]\n",
        "    )\n",
        "    rare_words = check_rare_words(Xpre, pre_vectorizer)\n",
        "\n",
        "    # import pdb; pdb.set_trace()\n",
        "    bow_vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "\n",
        "    not_rare_words = []\n",
        "\n",
        "    # rare_wordとstop_wordを確認する\n",
        "\n",
        "    dbg_cnt = 0 ###\n",
        "\n",
        "    for tmp_word_list in  df_tmp[\"epg_tknz\"].values:\n",
        "        tmp_list = []\n",
        "        for tmp_word in tmp_word_list:\n",
        "            # print(\"before_chck\"+str(tmp_word))\n",
        "            if tmp_word not in rare_words and tmp_word not in particular_stop_words:\n",
        "                print(\"after_chck\"+str(tmp_word))\n",
        "\n",
        "                # if tmp_word == \"genking\":\n",
        "                    # print(\"????why genking???\")\n",
        "                tmp_list.append(tmp_word)\n",
        "                dbg_cnt +=1###\n",
        "                \n",
        "        not_rare_words.append(tmp_list)\n",
        "        if dbg_cnt > 20 :###\n",
        "             ####break ### debug用\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "    X = bow_vectorizer.fit_transform( \n",
        "        [str(i) for i in not_rare_words] \n",
        "    )\n",
        "    X = bool_bow( X )\n",
        "\n",
        "    Y = df_tmp[\"win_lose_dummy\"]\n",
        "\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    clf = Lasso(alpha=0.1)\n",
        "\n",
        "    # 標準化する必要はない??\n",
        "    X = scaler.fit_transform(X)\n",
        "    \n",
        "    clf.fit(X, Y)\n",
        "\n",
        "    result_df = pd.DataFrame(clf.coef_.T , index=bow_vectorizer.vocabulary_ )\n",
        "\n",
        "    return result_df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay6I0J3GOj-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rare_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0gVhbP0R0gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HorizontalDisplay:\n",
        "    def __init__(self, *args):\n",
        "        self.args = args\n",
        "\n",
        "    def _repr_html_(self):\n",
        "        template = '<div style=\"float: left; padding: 10px;\">{0}</div>'\n",
        "        return \"\\n\".join(template.format(arg._repr_html_())\n",
        "                         for arg in self.args)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkyVJUnWOGNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######\n",
        "# early middle lateステージごとの結果を出す\n",
        "#\n",
        "\n",
        "def calc_lasso_by_stage (df_orijin):\n",
        "    df_early =  df_orijin[df_orijin[\"stages\"] == \"early\"]\n",
        "    df_middle =  df_orijin[df_orijin[\"stages\"] == \"middle\"]\n",
        "    df_late =  df_orijin[df_orijin[\"stages\"] == \"late\"]\n",
        "\n",
        "    result_late = fit_trans_lasso (df_late) \n",
        "    result_early = fit_trans_lasso (df_early) \n",
        "    result_middle = fit_trans_lasso (df_middle) \n",
        "\n",
        "    dfe = result_early[ result_early[0] != 0].sort_values(0)\n",
        "    dfm =result_middle[ result_middle[0] != 0].sort_values(0)\n",
        "    dfl =result_late[ result_late[0] != 0].sort_values(0)\n",
        "\n",
        "    # print(dfl)\n",
        "    display.display(HorizontalDisplay(dfe, dfm, dfl))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isY4VvJsNNda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calc_lasso_by_stage(df_stage[df_stage['drama_category'] ==\"love\"]  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEQUfLpi7-2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for tmp_e in df_stage[df_stage['drama_category'] ==\"love\"][\"epg_tknz\"]:\n",
        "    print(tmp_e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMw_4tEfzOyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# particular_stop_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU1DhN3jzOtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rare_words = []\n",
        "tmp_word = \"genking\"\n",
        "if tmp_word not in rare_words and tmp_word not in particular_stop_words:\n",
        "    print(\"c\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfbrNvNczOls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rare_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ0dj2kMOExl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "calc_lasso_by_stage(df_stage[df_stage['drama_category'] ==\"police\"]  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPzfnqV0L1t6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for tmp_e in df_stage[df_stage['drama_category'] ==\"police\"][\"epg_tknz\"]:\n",
        "    print(tmp_e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6gr8mpTmb40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_nan = df_stage[df_stage['drama_category'] ==\"nan\"]\n",
        "# win_lose_dummyにnanがあるので・・\n",
        "df_nan = df_nan[ ~pd.isnull(df_nan['win_lose_dummy'])]\n",
        "\n",
        "calc_lasso_by_stage(df_nan )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee6Ix-B9Uad7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import wordcloud, codecs\n",
        "wordc = wordcloud.WordCloud(font_path='HGRGM.TTC',\n",
        "        background_color='white',\n",
        "        mask=msk,\n",
        "        contour_color='steelblue',\n",
        "        contour_width=2).generate(splitted)\n",
        "wordc.to_file('sample-wordCloud-jpn.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW8zDwjVo2g_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################\n",
        "#\n",
        "#以下、授業の残り\n",
        "#\n",
        "####################\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B_7wgWRsa-9l",
        "colab": {}
      },
      "source": [
        "# ※これは、演習用に単語文書行列を DataFrame に変換して見やすくしてみるためのコードで、覚える必要はありません\n",
        "#pd.DataFrame(X.toarray(), columns=[ x[0] for x in sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1]) ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2moJhQosKWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}